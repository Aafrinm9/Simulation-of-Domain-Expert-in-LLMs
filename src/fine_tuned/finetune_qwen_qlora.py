# -*- coding: utf-8 -*-
"""qwen_finetuned_qlora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QazVCauaL9n0dm6rfMxPhu6HepnbA2QM
"""

# Check GPU availability
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

#Install required packages
!pip install bitsandbytes transformers>=4.35.0 peft>=0.8.0 datasets boto3

# Cell: Create S3 client with credentials
from google.colab import userdata
import boto3

# Get credentials from secrets
aws_access_key_id = userdata.get('AWS_ACCESS_KEY_ID')
aws_secret_access_key = userdata.get('AWS_SECRET_ACCESS_KEY')
aws_region = 'us-east-1'

# Create S3 client with explicit credentials
s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)

# Now use this s3_client for all your operations
try:
    # Test if it works
    bucket_name = 'finetune-train'
    response = s3_client.list_objects_v2(Bucket=bucket_name)
    print(f"Successfully connected to bucket. Found {len(response.get('Contents', []))} objects")
except Exception as e:
    print(f"Error: {e}")

# Continue with your data loading code
import logging
import os
import torch
import json
import numpy as np
from datasets import Dataset

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set device
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

# Load and normalize data using the s3_client we created above
all_examples = []

# Get the response again using our authenticated s3_client
response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='')

for item in response.get('Contents', []):
    if item['Key'].endswith('.json'):
        logger.info(f"Processing file: {item['Key']}")
        try:
            # Download the JSON file
            file_obj = s3_client.get_object(Bucket=bucket_name, Key=item['Key'])
            file_content = file_obj['Body'].read().decode('utf-8')
            data = json.loads(file_content)

            # Extract the document and conversation fields
            document = data.get("document", "")
            conversations = data.get("conversation", [])

            all_examples.append({
                "document": document,
                "conversation": conversations
            })

            logger.info(f"Successfully processed file: {item['Key']}")
        except Exception as e:
            logger.error(f"Error processing file {item['Key']}: {e}")

logger.info(f"Loaded {len(all_examples)} examples from JSON files")

#Data preprocessing:Create a Dataset object
dataset = Dataset.from_list(all_examples)

# Enhanced preprocessing function
def preprocess_data(example):
    document_context = example.get("document", "")
    conversations = example["conversation"]

    # Find question-response pairs
    pairs = []
    for i in range(0, len(conversations) - 1):
        if conversations[i]["role"] == "interviewer" and conversations[i+1]["role"] == "employee":
            question = conversations[i]
            response = conversations[i+1]

            question_quality = question.get("quality", "")

            pairs.append({
                "question": question["content"],
                "question_quality": question_quality,
            })

    # Create training examples
    examples = []
    for pair in pairs:
        if pair["question_quality"] == "high":
            examples.append({
                "context": document_context,
                "question": pair["question"],
                "is_good_question": True
            })

    return examples

# Process the dataset
processed_examples = []
for example in dataset:
    processed = preprocess_data(example)
    processed_examples.extend(processed)

# Create dataset from processed examples
processed_dataset = Dataset.from_list(processed_examples)
processed_dataset = processed_dataset.filter(lambda example: example["context"] != "")

# Log dataset statistics
logger.info(f"Processed dataset contains {len(processed_dataset)} examples")

# Create train/validation split
split_dataset = processed_dataset.train_test_split(test_size=0.1)
train_dataset = split_dataset["train"]
val_dataset = split_dataset["test"]
logger.info(f"Split dataset: {len(train_dataset)} training examples, {len(val_dataset)} validation examples")

# Cell: Tokenization
from transformers import AutoTokenizer

# Initialize tokenizer
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
logger.info(f"Loading tokenizer from {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def tokenize(example):
    # The data has 'question' column
    text = example["question"]

    # Tokenize the question text
    tokenized = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=512
    )

    # Create labels (copy of input_ids for language modeling)
    tokenized['labels'] = tokenized['input_ids'].copy()

    return tokenized

# Tokenize datasets
logger.info("Tokenizing datasets...")
train_tokenized = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)
val_tokenized = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)

# Set format for PyTorch
train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
val_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Cell: QLoRA configuration and model setup
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Define evaluation metrics
def compute_metrics(eval_preds):
    loss_array = eval_preds[0]
    loss_value = float(np.mean(loss_array))
    return {"loss": loss_value}

# QLoRA configuration - specify 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Load base model with QLoRA configuration
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Prepare model for k-bit training
base_model.gradient_checkpointing_enable()
base_model = prepare_model_for_kbit_training(base_model)

# QLoRA-specific LoRA configuration
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Get PEFT model for QLoRA
logger.info("Applying QLoRA adapter to base model")
model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

# Cell: Training configuration (fixed)
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
import os

# Define output directories
output_dir = "./qwen-0.5b-qlora-finetuned"
final_model_dir = "./final-qwen-0.5b-qlora"
os.makedirs(output_dir, exist_ok=True)
os.makedirs(final_model_dir, exist_ok=True)

# Update training arguments for QLoRA (fixed parameter names)
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    eval_strategy="steps",  # Changed from evaluation_strategy
    eval_steps=50,
    num_train_epochs=3,  # Reduced for Colab time limits
    learning_rate=2e-4,
    warmup_steps=50,
    weight_decay=0.01,
    save_steps=100,
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    gradient_checkpointing=True,
    report_to=["none"],  # Also fixed this format
    optim="paged_adamw_8bit",
    remove_unused_columns=False,
    bf16=True
)

# Initialize trainer
logger.info("Initializing Trainer")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    compute_metrics=compute_metrics,
)

# Fine-tune the model
logger.info("Starting training...")
trainer.train()

# Save training results
training_results = {
    "final_loss": trainer.state.log_history[-1].get("loss", None),
    "best_eval_loss": trainer.state.best_metric,
    "total_steps": trainer.state.global_step,
    "optimizer": "QLoRA"
}

import json
with open(os.path.join(output_dir, "training_results.json"), "w") as f:
    json.dump(training_results, f, indent=4)

# Save the adapter weights
logger.info("Saving QLoRA adapter weights...")
adapter_save_path = "./qwen-0.5b-qlora-adapter"
model.save_pretrained(adapter_save_path)
tokenizer.save_pretrained(adapter_save_path)

logger.info("QLoRA fine-tuning complete!")
logger.info(f"Training results saved to: {output_dir}/training_results.json")
logger.info(f"QLoRA adapter: {adapter_save_path}")

# Create a zip file of the entire model folder
!zip -r qwen-qlora-model.zip qwen-0.5b-qlora-adapter

# Download the zip file
from google.colab import files
files.download('qwen-qlora-model.zip')

